{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b539437",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) Prerequisites\n",
    "___\n",
    "## Notes\n",
    "In this notebook, we will discuss some of the fundamental (or prerequisite) knowledge needed to be successful in NLP. \n",
    "\n",
    "Specifically, we'll be talking about the following:\n",
    "> [Natural Language Toolkit (NLTK)](#Natural-Language-Toolkit-(NLTK))\n",
    ">\n",
    "> [Reading Text Data](#Reading-Text-Data)\n",
    ">\n",
    "> [Understanding the Data](#Understanding-the-Data)\n",
    ">\n",
    "> [Regular Expressions (RegEx)](#Regular-Expression-(RegEx))\n",
    ">\n",
    "> [Stemming](#Stemming)\n",
    ">\n",
    "> [Lemmatizing](#Lemmatizing)\n",
    "\n",
    "We finish the notebook off with a [review](#Review) of everything discussed, as well as instructions to access the guided assignment associated with this material. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a590c5c9",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f2193",
   "metadata": {},
   "source": [
    "## Natural Language Toolkit (NLTK)\n",
    "> **NLTK** is a suite of open-source tools created to make NLP processing in Python easier. For installation instructions, see https://www.nltk.org/install.html.\n",
    ">\n",
    ">Below, we'll mess around with NLTK a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649a1f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download packages/corpora:\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23916721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see what you have installed:\n",
    "dir(nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f35298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see stopwords (common words that don't have much meaning in the sentence):\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571dbdf0",
   "metadata": {},
   "source": [
    "There is plenty more, so feel free to experiment on your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0350f907",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fbf66a",
   "metadata": {},
   "source": [
    "## Reading Text Data:\n",
    "> Without data, we can't do any processing on it. In this section, we focus on how to load in data.\n",
    ">\n",
    "> We also take a look at **Pandas** and see how we can load data into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab3e293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in file:\n",
    "file = open('filename.txt').read()\n",
    "\n",
    "# Print some of the data in the file:\n",
    "file[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7502d2",
   "metadata": {},
   "source": [
    "Imagine each line in the file has a label followed by a sentence separated by a tab. We can break the label and sentence up to create a list that's easier to work with by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b616b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_file = file.replace('\\t','\\n').split('\\n')\n",
    "split_file[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69808857",
   "metadata": {},
   "source": [
    "We separate the labels and sentences as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b50ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = split_data[0::2]\n",
    "sentences = split_file[1::2]\n",
    "print('Labels:\\n',labels)\n",
    "print('Sentences:\\n',sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7da91bb",
   "metadata": {},
   "source": [
    "We will be using Pandas dataframes to work with a corpus (a collection of words). For more information on Pandas, see LINK. \n",
    "\n",
    "Here, we'll create a Pandas dataframe and ###. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27837f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "corpus = pd.DataFrame({\n",
    "    'labels':labels,\n",
    "    'sentences':sentences\n",
    "})\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac859e5d",
   "metadata": {},
   "source": [
    "Note that Pandas can actually load in a tab-delineated file (as the one described above), so let's see how we can do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fc2683",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('filename.txt', sep='\\t', header=None)\n",
    "corpus.columns = ['labels','sentences']\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6535e684",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d761f6",
   "metadata": {},
   "source": [
    "## Understanding the Data\n",
    "> Now that we've loaded out data, we want to take a closer look at it. \n",
    "> \n",
    "> How is the data shaped/structured? When working with labeled data, how many samples correspond to each label? Are there more samples of one label than samples of the other label(s)? Are we missed any data? \n",
    ">\n",
    "> Let's dive in. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b99f819",
   "metadata": {},
   "source": [
    "We start by looking at the shape of our corpus, printing out the number of samples (rows) and the number of features (columns) of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb139e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Our corpus has {} samples with {} features.'.format(len(corpus),len(corpus.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526fbe1d",
   "metadata": {},
   "source": [
    "Next, we look to see how many samples (rows) correspond to each label (first feature of our data). Assume the data we were working with had labels 'confidential' and 'non-confidential'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed053195",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} samples labeled confidential and {} samples labeled non-confidential'.format((corpus['labels']=='confidential').sum(),\n",
    "                                                                                                 (corpus['labels']=='non-confidential').sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4622c5",
   "metadata": {},
   "source": [
    "Now, let's see how many (if any) samples are missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45792915",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# of missing labels: {}'.format(corpus['labels'].isnull().sum()))\n",
    "print('# of missing sentences: {}'.format(corpus['sentences'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c7ec3",
   "metadata": {},
   "source": [
    "Look at the outputs above. Was your data skewed? Was there missing data? We will want to keep that in mind moving forward.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba897ec",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43485741",
   "metadata": {},
   "source": [
    "## Regular Expression (RegEx)\n",
    "> **Regular expressions** are text strings that describe a search pattern. We will use regular expressions to simplify our data searches. \n",
    "> \n",
    "> Regular expressions will prove to be very helpful when dealing with unstructured data. In those cases, we wlll have to search for specific patterns to try and create meaning from the data. \n",
    ">\n",
    "> Here are a couple of examples that regular expressions can help us with: \n",
    "> - identifying whitespace between words/tokens\n",
    "> - removing punctuation from text\n",
    "> - cleaning HTML tags from text\n",
    "> - confirming text meets a specific criteria (such as a password)\n",
    "> - and many more! \n",
    "> \n",
    "> Some commonly used regex are shown below: \n",
    "> INSERT TABLE>\n",
    "> \n",
    "> For a more extensive cheat sheet, see #. \n",
    "> \n",
    "> Below, we will use regex to **tokenize** strings. **Tokenization** refers to separating text into smaller units called tokens, which can be either words, characters, or subwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b046296",
   "metadata": {},
   "source": [
    "To use regex in Python, we  make use of the `re` package. \n",
    "\n",
    "Let's start by importing this package and then make a few strings to use regular expressions with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a3476fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "string1 = 'I am excited to be learning natural language processing'\n",
    "string2 = 'I        am excited to be         learning         natural language processing'\n",
    "string3 = 'I|am|excited|to|be|learning|>>>>>|natural|language|processing'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62989f07",
   "metadata": {},
   "source": [
    "What if we wanted to split the strings up by whitespace? We can use the `re.split` method, using `'\\s'` to split the string on a whitespace character (recall the table above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "081a3cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'excited', 'to', 'be', 'learning', 'natural', 'language', 'processing'] \n",
      "\n",
      "['I', '', '', '', '', '', '', '', 'am', 'excited', 'to', 'be', '', '', '', '', '', '', '', '', 'learning', '', '', '', '', '', '', '', '', 'natural', 'language', 'processing'] \n",
      "\n",
      "['I|am|excited|to|be|learning|>>>>>|natural|language|processing'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(re.split('\\s', string1),'\\n')\n",
    "print(re.split('\\s', string2),'\\n')\n",
    "print(re.split('\\s', string3),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9201d3d9",
   "metadata": {},
   "source": [
    "We see that splitting by whitespace only helped us for `string1`. What regular expression would help us with `string2` and `string3`? \n",
    "\n",
    "Well, `string2` has multiple whitespaces in succession, so perhaps `'\\s+'` would handle those. And `string3` has non-alphanumeric characters, so perhaps `'\\W+'` would handle those. Let's try this out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4029bc08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'excited', 'to', 'be', 'learning', 'natural', 'language', 'processing'] \n",
      "\n",
      "['I', 'am', 'excited', 'to', 'be', 'learning', 'natural', 'language', 'processing'] \n",
      "\n",
      "['I', 'am', 'excited', 'to', 'be', 'learning', 'natural', 'language', 'processing'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(re.split('\\s', string1),'\\n')\n",
    "print(re.split('\\s+', string2),'\\n')\n",
    "print(re.split('\\W+', string3),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5295f7d3",
   "metadata": {},
   "source": [
    "Instead of splitting the strings, let's try using the `re.findall` method to obtain the same results. In this scenario, we will have to use negating regex compared to when we used `re.split`. Does this make sense?\n",
    "\n",
    "For example, we want to find all sets of non-whitespace characters for `string1` and `string2` (note that `string1`'s regex needs a `+` in it to get said set of characters). Similarly, we want to find all sets of alphanumeric characters for `string3`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd4d0da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'excited', 'to', 'be', 'learning', 'natural', 'language', 'processing'] \n",
      "\n",
      "['I', 'am', 'excited', 'to', 'be', 'learning', 'natural', 'language', 'processing'] \n",
      "\n",
      "['I', 'am', 'excited', 'to', 'be', 'learning', 'natural', 'language', 'processing'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(re.findall('\\S+', string1),'\\n')\n",
    "print(re.findall('\\S+', string2),'\\n')\n",
    "print(re.findall('\\w+', string3),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1661eaf2",
   "metadata": {},
   "source": [
    "Awesome! We just learned how to **tokenize** strings using the methods `re.split()` and `re.findall()`. \n",
    "\n",
    "There are plenty of other regex methods that will prove to be useful, such as:\n",
    "- `re.sub()`\n",
    "- `re.search()`\n",
    "- `re.match()`\n",
    "- `re.fullmatch()`\n",
    "- `re.finditer()`\n",
    "- `re.escape()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ed4032",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116de3bf",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "> **Stemming** refers to the process of reducing a word to its stem, or root. Typically, this means chopping off the end of the word to the point where only the base remains. \n",
    ">\n",
    "> Some examples of stemming include:\n",
    "> - testing -> test\n",
    "> - cows -> cow\n",
    "> - connection -> connect\n",
    "> \n",
    "> Stemming is important for NLP because it allows us to reduce the corpus of words our model needs to work with while simulatenously correlating, explicitly, words with similar meaning. \n",
    "> \n",
    "> While there are a handful of stemmers out there (Snowball, Lancaster, Regex-Based), we will focus on the Porter stemmer as it is the most popular. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7093e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "ps = nltk.PorterStemmer()\n",
    "# dir(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fb3bc472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['grow', 'grow', 'grow']\n",
      "['mean', 'mean', 'mean', 'meaning']\n",
      "['test', 'test', 'test', 'tester', 'testimoni']\n",
      "['goos', 'gees']\n"
     ]
    }
   ],
   "source": [
    "print([ps.stem(x) for x in ['grow','growing','grows']])\n",
    "print([ps.stem(x) for x in ['mean','meanness','meaning','meaningful']])\n",
    "print([ps.stem(x) for x in ['test','testing','tested','tester','testimony']])\n",
    "print([ps.stem(x) for x in ['goose','geese']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992664f0",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69875774",
   "metadata": {},
   "source": [
    "## Lemmatizing\n",
    "> **Lemmatizing** refers to the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. \n",
    "> \n",
    "> Note that lemmatizing is very similar to stemming. They both aim to condense derived words into their base forms, but stemming does so heuristically (practically, not opmtimally) without the context in which the word was used. \n",
    "> \n",
    "> Stemming can result in a non-dictionary word, such as 'testimoni' shown above, whereas lemmatization will always result in a dictionary word. \n",
    ">\n",
    "> So, why don't we always lemmatize? Unfortunately, lemmatizing is more computationally complex than stemming and therefore there exists this trade-off between accuracy and computational expense. \n",
    ">\n",
    "> There are also a couple of lemmatizers out there, but we'll be using the WordNet lemmatizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "351a8357",
   "metadata": {},
   "outputs": [],
   "source": [
    "wn = nltk.WordNetLemmatizer()\n",
    "# dir(wn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "03be3853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['grow', 'growing', 'grows']\n",
      "['mean', 'meanness', 'meaning', 'meaningful']\n",
      "['test', 'testing', 'tested', 'tester', 'testimony']\n",
      "['goose', 'goose']\n"
     ]
    }
   ],
   "source": [
    "print([wn.lemmatize(x) for x in ['grow','growing','grows']])\n",
    "print([wn.lemmatize(x) for x in ['mean','meanness','meaning','meaningful']])\n",
    "print([wn.lemmatize(x) for x in ['test','testing','tested','tester','testimony']])\n",
    "print([wn.lemmatize(x) for x in ['goose','geese']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ec38b1",
   "metadata": {},
   "source": [
    "We emphasize the computational complexity of our lemmatizer compared to our stemmer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "a83daded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to stem: 0.2281665802001953 ms\n",
      "Time taken to lemmatize: 0.11897087097167969 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "stem_example = [ps.stem(x) for x in ['test','testing','tested','tester','testimony']]\n",
    "end = time.time()\n",
    "print('Time taken to stem: {} ms'.format((end-start)*float(1000)))\n",
    "\n",
    "start = time.time()\n",
    "lemmatize_example = [wn.lemmatize(x) for x in ['test','testing','tested','tester','testimony']]\n",
    "end = time.time()\n",
    "print('Time taken to lemmatize: {} ms'.format((end-start)*float(1000)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d106e7c",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d667be08",
   "metadata": {},
   "source": [
    "## Review\n",
    "> Before we even start thinking about plugging out data into machine learning models, we typically have to perform these steps on our data first:\n",
    "> 1. Remove Punctuation\n",
    "> 2. Tokenize\n",
    "> 3. Remove Stopwords\n",
    "> 4. Lemmatize/Stem\n",
    "> \n",
    "> Our **NLP Prerequisites - Guided Assignment** can be used to check that we know how to do all of these things before moving on. Give it a go, making sure to compare your solution with the one given ONLY AFTER you've completed the assignment on your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246e9096",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
