{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b02ed578",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "Adapted from \"NLP with Python for Machine Learning Essential Training\" by Derek Jedamski. \n",
    "___\n",
    "## Notes\n",
    "In this notebook, we will discuss machine learning as a whole.\n",
    "\n",
    "Specifically, we'll be talking about the following:\n",
    "> [Overview](#Overview:-Machine-Learning)\n",
    ">\n",
    "> [System Evaluation](#)\n",
    ">\n",
    "> [Random Forests](#)\n",
    ">\n",
    "> [Gradient Boosting](#)\n",
    "\n",
    "We finish the notebook off with a [review](#Review) of everything discussed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f8d3d6",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a5ff14",
   "metadata": {},
   "source": [
    "> ## Overview: Machine Learning\n",
    "> ___\n",
    "> **Machine Learning** refers to algorithms that use data to make predictions. \n",
    ">\n",
    "> There are two types of machine learning: \n",
    "> - **Supervised Learning**: Learning a mapping function from labeled training data to make predictions on unseen data.\n",
    "> \n",
    "> - **Unsupervised Learning**: Discovering hidden patterns/structures in unlabelled data.\n",
    "> \n",
    "> We will be focusing on supervised learning for now. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8756ba",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea73e46",
   "metadata": {},
   "source": [
    "> ## System Evaluation\n",
    "> ___\n",
    "> **k-Fold Cross Validation**.\n",
    "> \n",
    "> **Evaluation metrics** quantify the performance of our predictive model. \n",
    ">\n",
    "> Here are a few common evaluation metrics we will use:\n",
    "> - $\\text{Accuracy} = \\frac{\\text{# predicted correctly}}{\\text{ total # of observations}}$\n",
    "> - $\\text{Precision} = \\frac{\\text{# predicted as class A that are actually class A}}{\\text{ total # predicted as class A}}$\n",
    "> - $\\text{Recall} = \\frac{\\text{# predicted as class A that are actually class A}}{\\text{ total # that are actually class A}}$\n",
    ">\n",
    "> `Note`: If we want to limit false positives, we will optimize the model for precision. If we want to limit false negatives, we will optomize the model for recall. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a18e88b",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757fe0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79bc89fc",
   "metadata": {},
   "source": [
    "> ## Random Forests\n",
    "> \n",
    "> **Random forests** are collections of decision trees whose predictions aggregated to obtain a final prediction. \n",
    "> \n",
    "> They use **ensemble learning**, a process by which multiple models are created and combined to make a more informed prediction (compared to a single model). \n",
    ">\n",
    "> As with any other model, we can use things like **k-fold cross validation** and **train-test splits** to evaluate our random forests. We can also use things like **grid-search**, an exhaustive search process across all parameter combinations in a given grid for the best performing parameters, to improve our random forests. \n",
    ">\n",
    "> Let's explore random forests a bit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e5270f",
   "metadata": {},
   "source": [
    "First, we'll read in our data and clean it up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daf2f6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "data = pd.read_csv('SMSSpamCollection.tsv', sep='\\t', header=None)\n",
    "data.columns = ['label','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3427bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_punctuation(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")),3)*100\n",
    "\n",
    "data['length'] = data['text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "data['punctuation'] = data['text'].apply(lambda x: count_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1375ab19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>length</th>\n",
       "      <th>punctuation</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>8181</th>\n",
       "      <th>8182</th>\n",
       "      <th>8183</th>\n",
       "      <th>8184</th>\n",
       "      <th>8185</th>\n",
       "      <th>8186</th>\n",
       "      <th>8187</th>\n",
       "      <th>8188</th>\n",
       "      <th>8189</th>\n",
       "      <th>8190</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>160</td>\n",
       "      <td>2.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>128</td>\n",
       "      <td>4.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>49</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62</td>\n",
       "      <td>3.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>7.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 8193 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   length  punctuation    0    1    2    3    4    5    6    7  ...  8181  \\\n",
       "0     160          2.5  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "1     128          4.7  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "2      49          4.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "3      62          3.2  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "4      28          7.1  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...   0.0   \n",
       "\n",
       "   8182  8183  8184  8185  8186  8187  8188  8189  8190  \n",
       "0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "1   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "2   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "3   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "4   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[5 rows x 8193 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = ''.join([char for char in text if char not in string.punctuation])\n",
    "    tokenized_text = re.split('\\W+',cleaned_text)\n",
    "    stemmed_tokens = [ps.stem(word) for word in tokenized_text if word not in stopwords]\n",
    "    return stemmed_tokens\n",
    "\n",
    "tdidf_vect = TfidfVectorizer(analyzer=clean_text)\n",
    "X_tfidf_counts = tdidf_vect.fit_transform(data['text'])\n",
    "\n",
    "X_features = pd.concat([data['length'], data['punctuation'], pd.DataFrame(X_tfidf_counts.toarray())], axis=1)\n",
    "X_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b2255",
   "metadata": {},
   "source": [
    "Now, let's import our `RandomForestClassifier` and explore it through 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47819d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98384201, 0.96947935, 0.97666068, 0.98743268, 0.97307002,\n",
       "       0.98025135, 0.97127469, 0.96947935, 0.9676259 , 0.98201439])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1) # allows for trees to run in parallel\n",
    "k_fold = KFold(n_splits=10)\n",
    "cross_val_score(rf, X_features, data['label'], cv=k_fold, scoring='accuracy', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9696ac37",
   "metadata": {},
   "source": [
    "Let's explore our `RandomForestClassifier` using a holdout test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "12df01a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, data['label'], test_size = 0.2)\n",
    "rf2 = RandomForestClassifier(n_estimators= 50,max_depth=20, n_jobs=-1)\n",
    "rf2_model = rf2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06221037",
   "metadata": {},
   "source": [
    "Our `RandomForestClassifier` has a `feature_importances_` attribute we would like to make use of. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ca43a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.04727789288599296, 'length'),\n",
       " (0.039905742377050364, 2048),\n",
       " (0.02900918223533704, 1819),\n",
       " (0.0245632355485361, 7090),\n",
       " (0.023629581186565912, 4838),\n",
       " (0.02316410225353268, 3159),\n",
       " (0.021391812063942965, 295),\n",
       " (0.020058904539315043, 7422),\n",
       " (0.019354545728288715, 2188),\n",
       " (0.017632349642892228, 7864)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(zip(rf2_model.feature_importances_, X_train.columns), reverse=True)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ec8c9d",
   "metadata": {},
   "source": [
    "The above results tell us that the length of text is a key factor in determining labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c6a196",
   "metadata": {},
   "source": [
    "Let's go ahead and make predictions using our model (since we've already fit the model to our training data). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bde6a300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 1.0\n",
      "Recall: 0.57\n",
      "Accuracy: 0.942\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf2_model.predict(X_test)\n",
    "precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "print('Precision: {}'.format(round(precision,3)))\n",
    "print('Recall: {}'.format(round(recall,3)))\n",
    "print('Accuracy: {}'.format(round((y_pred==y_test).sum() / len(y_pred),3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b7f4e4",
   "metadata": {},
   "source": [
    "The above results tell us that:\n",
    "- 100% mail in the spam folder is actually spam\n",
    "- 57% of all spam was properly marked as spam\n",
    "- 94.2% of all emails were identified correctly (whether spam or ham)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec788ed",
   "metadata": {},
   "source": [
    "To finish up, let's perform a grid-search for our `RandomForestClassifier`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff2065f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Est: 10 / Depth: 10 --- Precision: 1.0 / Recall: 0.146 / Accuracy: 0.884\n",
      "Est: 10 / Depth: 20 --- Precision: 1.0 / Recall: 0.57 / Accuracy: 0.942\n",
      "Est: 10 / Depth: 30 --- Precision: 1.0 / Recall: 0.742 / Accuracy: 0.965\n",
      "Est: 10 / Depth: None --- Precision: 0.991 / Recall: 0.755 / Accuracy: 0.966\n",
      "Est: 50 / Depth: 10 --- Precision: 1.0 / Recall: 0.238 / Accuracy: 0.897\n",
      "Est: 50 / Depth: 20 --- Precision: 1.0 / Recall: 0.55 / Accuracy: 0.939\n",
      "Est: 50 / Depth: 30 --- Precision: 1.0 / Recall: 0.675 / Accuracy: 0.956\n",
      "Est: 50 / Depth: None --- Precision: 1.0 / Recall: 0.801 / Accuracy: 0.973\n",
      "Est: 100 / Depth: 10 --- Precision: 1.0 / Recall: 0.258 / Accuracy: 0.899\n",
      "Est: 100 / Depth: 20 --- Precision: 1.0 / Recall: 0.536 / Accuracy: 0.937\n",
      "Est: 100 / Depth: 30 --- Precision: 1.0 / Recall: 0.682 / Accuracy: 0.957\n",
      "Est: 100 / Depth: None --- Precision: 1.0 / Recall: 0.815 / Accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "def train_rf(num_estimators, depth_num):\n",
    "    rf = RandomForestClassifier(n_estimators=num_estimators, max_depth=depth_num, n_jobs=-1)\n",
    "    rf_model = rf.fit(X_train, y_train)\n",
    "    y_pred = rf_model.predict(X_test)\n",
    "    precision, recall, fscore, support = score(y_test, y_pred, pos_label='spam', average='binary')\n",
    "    print('Est: {} / Depth: {} --- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "        num_estimators, depth_num, round(precision,3), round(recall,3), \n",
    "        round((y_pred==y_test).sum() / len(y_pred),3)))\n",
    "\n",
    "for num_estimators in [10, 50, 100]:\n",
    "    for depth_num in [10, 20, 30, None]:\n",
    "        train_rf(num_estimators, depth_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54baab74",
   "metadata": {},
   "source": [
    "The above results tell us that:\n",
    "- As depth increase, recall increases drastically without a change in precision\n",
    "- As estimators increase, recall increases slightly without a change in precision\n",
    "\n",
    "Therefore, the best random forest model for this problem would be one with a very high depth. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2635161e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2625f6b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d99eb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf414576",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9618a37a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafd1929",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f90218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a2c3bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
