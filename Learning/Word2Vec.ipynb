{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec34eb5b",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "___\n",
    "## Notes\n",
    "In this notebook, we will discuss the word2vec algorithm.\n",
    "\n",
    "Specifically, we break our discussion down into the following sections:\n",
    "> [Overview](#Overview:-word2vec)\n",
    ">\n",
    "> [Implementation](#Implementation)\n",
    ">\n",
    ">> [Pre-Trained Embeddings](#Pre-Trained-Embeddings)\n",
    ">>\n",
    ">> [Training Embeddings](#Training-Embeddings)\n",
    ">>\n",
    ">> [Preparing Word Vectors for ML Models](#Preparing-Word-Vectors-for-ML-Models)\n",
    "\n",
    "We finish the notebook off with a [review](#Review) of everything discussed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7807b18e",
   "metadata": {},
   "source": [
    "## Overview\n",
    "> **Word2Vec** is an embedding algorithm based on a shllow, two-layer neural network that takes a text corpus as input and outputs a vector representation for each word in the corpus. \n",
    "> \n",
    "> There are many ways to train a word2vec mode, but for now we will look at the **skip-gram** method, which looks at a window of words to the left and right of a given word to determine its context and map it into a vector space. \n",
    "> \n",
    "> This idea is based on the saying: \"*you shall know a word by the company it keeps.*\" \n",
    ">\n",
    "> From this vector representation, we can determine **word similarity**. A popular way to calculate word similarity is via **cosine similarity**, which determines the cosine value of the angle between the two word vectors you are trying to determine the similarity of. Therefore, if the angle between the word vectors is small, the similarity is very high. \n",
    ">\n",
    "> There vector representations also give way to the construction of word analogies. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6d7fbf",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "> When using word2vec, we can either:\n",
    "> 1. use pre-trained embeddings, where a word2vec model has already been trained on a large corpus of text:\n",
    ">     - `glove-twitter-{25/50/100/200}`\n",
    ">     - `glove-wiki-gigaword-{50/100/200/300}`\n",
    ">     - `word2vec-google-news-300`\n",
    ">     - `word2vec-ruscorpora-news-300`\n",
    ">     - and a few [others](https://radimrehurek.com/gensim/models/word2vec.html)...\n",
    "> 2. train embeddings using our own set of data.\n",
    ">\n",
    "> Once we have our embeddings, we need to do a bit more prep word to get the vector representations ready for input into a machine learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c8936c",
   "metadata": {},
   "source": [
    "### Pre-Trained Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bced4a8d",
   "metadata": {},
   "source": [
    "We start by importing `gensim`, a package that comes with a bunch of pre-trained embeddings build in, and loading the `glove-wiki-gigaword-100` embedding (the 100 represents the length each vector should be in the embedding). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cdb6177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f004c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "wiki_embeddings = api.load('glove-wiki-gigaword-100')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fd2afb",
   "metadata": {},
   "source": [
    "Now, we'll examine the word vector for the word \"king\", and find words most similar to \"king\" based on our embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9520f5f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.32307 , -0.87616 ,  0.21977 ,  0.25268 ,  0.22976 ,  0.7388  ,\n",
       "       -0.37954 , -0.35307 , -0.84369 , -1.1113  , -0.30266 ,  0.33178 ,\n",
       "       -0.25113 ,  0.30448 , -0.077491, -0.89815 ,  0.092496, -1.1407  ,\n",
       "       -0.58324 ,  0.66869 , -0.23122 , -0.95855 ,  0.28262 , -0.078848,\n",
       "        0.75315 ,  0.26584 ,  0.3422  , -0.33949 ,  0.95608 ,  0.065641,\n",
       "        0.45747 ,  0.39835 ,  0.57965 ,  0.39267 , -0.21851 ,  0.58795 ,\n",
       "       -0.55999 ,  0.63368 , -0.043983, -0.68731 , -0.37841 ,  0.38026 ,\n",
       "        0.61641 , -0.88269 , -0.12346 , -0.37928 , -0.38318 ,  0.23868 ,\n",
       "        0.6685  , -0.43321 , -0.11065 ,  0.081723,  1.1569  ,  0.78958 ,\n",
       "       -0.21223 , -2.3211  , -0.67806 ,  0.44561 ,  0.65707 ,  0.1045  ,\n",
       "        0.46217 ,  0.19912 ,  0.25802 ,  0.057194,  0.53443 , -0.43133 ,\n",
       "       -0.34311 ,  0.59789 , -0.58417 ,  0.068995,  0.23944 , -0.85181 ,\n",
       "        0.30379 , -0.34177 , -0.25746 , -0.031101, -0.16285 ,  0.45169 ,\n",
       "       -0.91627 ,  0.64521 ,  0.73281 , -0.22752 ,  0.30226 ,  0.044801,\n",
       "       -0.83741 ,  0.55006 , -0.52506 , -1.7357  ,  0.4751  , -0.70487 ,\n",
       "        0.056939, -0.7132  ,  0.089623,  0.41394 , -1.3363  , -0.61915 ,\n",
       "       -0.33089 , -0.52881 ,  0.16483 , -0.98878 ], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_embeddings['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e79e204",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('prince', 0.7682329416275024),\n",
       " ('queen', 0.7507690787315369),\n",
       " ('son', 0.7020888328552246),\n",
       " ('brother', 0.6985775828361511),\n",
       " ('monarch', 0.6977890729904175),\n",
       " ('throne', 0.691999077796936),\n",
       " ('kingdom', 0.6811409592628479),\n",
       " ('father', 0.6802029013633728),\n",
       " ('emperor', 0.6712858080863953),\n",
       " ('ii', 0.6676074266433716)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_embeddings.most_similar('king')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3cf1b5",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749ebde5",
   "metadata": {},
   "source": [
    "### Training Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8184661b",
   "metadata": {},
   "source": [
    "Let's try to train our own embeddings using the `SMSSpamCollection.tsv` file. Start by loading in, and cleaning up, the file. \n",
    "\n",
    "While we could clean the data ourselves, we'll make use of the `gensim` `simple_preprocess` function. This will remove punctuation and stopwords before tokenizing the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45a4f1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>I've been searching for the right words to thank you for this breather. I promise i wont take yo...</td>\n",
       "      <td>[ve, been, searching, for, the, right, words, to, thank, you, for, this, breather, promise, wont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "      <td>[free, entry, in, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "      <td>[nah, don, think, he, goes, to, usf, he, lives, around, here, though]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>Even my brother is not like to speak with me. They treat me like aids patent.</td>\n",
       "      <td>[even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>I HAVE A DATE ON SUNDAY WITH WILL!!</td>\n",
       "      <td>[have, date, on, sunday, with, will]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1  spam   \n",
       "2   ham   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text  \\\n",
       "0  I've been searching for the right words to thank you for this breather. I promise i wont take yo...   \n",
       "1  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...   \n",
       "2                                        Nah I don't think he goes to usf, he lives around here though   \n",
       "3                        Even my brother is not like to speak with me. They treat me like aids patent.   \n",
       "4                                                                  I HAVE A DATE ON SUNDAY WITH WILL!!   \n",
       "\n",
       "                                                                                            clean_text  \n",
       "0  [ve, been, searching, for, the, right, words, to, thank, you, for, this, breather, promise, wont...  \n",
       "1  [free, entry, in, wkly, comp, to, win, fa, cup, final, tkts, st, may, text, fa, to, to, receive,...  \n",
       "2                                [nah, don, think, he, goes, to, usf, he, lives, around, here, though]  \n",
       "3         [even, my, brother, is, not, like, to, speak, with, me, they, treat, me, like, aids, patent]  \n",
       "4                                                                 [have, date, on, sunday, with, will]  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth',100)\n",
    "\n",
    "messages = pd.read_csv('SMSSpamCollection.tsv', sep='\\t', header=None)\n",
    "messages.columns = ['label','text']\n",
    "\n",
    "messages['clean_text'] = messages['text'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bba070",
   "metadata": {},
   "source": [
    "Now, go ahead and split our data up into train and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "516835ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(messages['clean_text'],messages['label'], test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c185133",
   "metadata": {},
   "source": [
    "We need to train our word2vec model now. For now, we'll set:\n",
    "- `vector_size=100`: the length of the word embedding vectors (aka the dimensions the word gets mapped into)\n",
    "- `window=5`: the number of words to look at for context (5 would mean 2 to the left and 2 to the right of the current word)\n",
    "- `min_count=2`: the number of times a word needs to appear in the corpus to create a vector embedding for that word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe9b0e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = gensim.models.Word2Vec(X_train, vector_size=100, window=5, min_count=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24699ec2",
   "metadata": {},
   "source": [
    "Now, we'll examine the word vector for the word \"king\", and find words most similar to \"king\" based on our embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4f4434ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04996508,  0.06191731,  0.01920232, -0.00921991, -0.00794392,\n",
       "       -0.11238229,  0.03144125,  0.15554667, -0.04757956, -0.06054191,\n",
       "       -0.02493219, -0.08994311, -0.02334521,  0.03916509,  0.02084255,\n",
       "       -0.03985299, -0.0059781 , -0.0659265 , -0.00192455, -0.13866237,\n",
       "        0.033167  ,  0.04735364,  0.03730169, -0.02935965, -0.00567448,\n",
       "       -0.01049724, -0.04019884, -0.05439038, -0.07118474,  0.03066384,\n",
       "        0.07288796, -0.00085214,  0.03131102, -0.05915679, -0.03692801,\n",
       "        0.08658414,  0.04561409, -0.05220363, -0.01140274, -0.1216219 ,\n",
       "        0.02775528, -0.04251336, -0.0558888 , -0.00066932,  0.05631372,\n",
       "       -0.04125058, -0.04528016, -0.01465464,  0.05670632,  0.03521145,\n",
       "        0.04856405, -0.06812292, -0.00350109, -0.00766639, -0.0172387 ,\n",
       "        0.05529414,  0.04766516,  0.00591104, -0.09423443,  0.02206806,\n",
       "        0.01064999, -0.00531434, -0.00255395, -0.01964384, -0.07075865,\n",
       "        0.06811004,  0.02430508,  0.07080574, -0.07174548,  0.07144115,\n",
       "       -0.04561865,  0.06563386,  0.09413301, -0.01821867,  0.06079134,\n",
       "        0.03127601,  0.01014564, -0.02310837, -0.06519237, -0.0004825 ,\n",
       "       -0.05120867, -0.01404801, -0.0786704 ,  0.10532764, -0.01349606,\n",
       "       -0.02429941,  0.02761989,  0.06669828,  0.08160085,  0.02958917,\n",
       "        0.11060236,  0.05135116,  0.00837148,  0.03927771,  0.13166027,\n",
       "        0.06113048,  0.04012184, -0.04422384,  0.00614625, -0.04140215],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv['king']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f4ba9455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('again', 0.9946290850639343),\n",
       " ('girl', 0.9946222901344299),\n",
       " ('think', 0.9945281744003296),\n",
       " ('there', 0.9945186376571655),\n",
       " ('done', 0.9944351315498352),\n",
       " ('same', 0.9944155812263489),\n",
       " ('why', 0.9943594336509705),\n",
       " ('wait', 0.9943505525588989),\n",
       " ('in', 0.9943446516990662),\n",
       " ('very', 0.9943395256996155)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('king')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d66117c",
   "metadata": {},
   "source": [
    "`Note`: This embedding don't make as much sense as the one we obtained from the pre-trained model earlier. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95bd6ad",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570d6e73",
   "metadata": {},
   "source": [
    "### Preparing Word Vectors for ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab56521a",
   "metadata": {},
   "source": [
    "First, we'll take a look at the words that have embeddings in our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "def8a42b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to',\n",
       " 'you',\n",
       " 'the',\n",
       " 'and',\n",
       " 'is',\n",
       " 'in',\n",
       " 'me',\n",
       " 'it',\n",
       " 'my',\n",
       " 'for',\n",
       " 'your',\n",
       " 'of',\n",
       " 'call',\n",
       " 'have',\n",
       " 'that',\n",
       " 'on',\n",
       " 'now',\n",
       " 'are',\n",
       " 'can',\n",
       " 'so',\n",
       " 'not',\n",
       " 'but',\n",
       " 'we',\n",
       " 'or',\n",
       " 'at']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.index_to_key[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7b43f9",
   "metadata": {},
   "source": [
    "Now, let's generate a vector for each message in the training data based on the word vectors for each word in that message. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "db41756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vect = np.array([np.array([w2v_model.wv[i] for i in msg if i in w2v_model.wv.index_to_key]) for msg in X_train], dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ffeef4",
   "metadata": {},
   "source": [
    "A machine learning model will need the same set of features for each example it sees. In our case, each word is a feature, and the fact that messages have different numbers of words will cause issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e199da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 7\n",
      "75 23\n",
      "21 23\n",
      "12 3\n",
      "25 16\n",
      "6 5\n",
      "15 17\n",
      "25 21\n",
      "6 5\n",
      "18 11\n",
      "38 58\n",
      "43 8\n",
      "10 30\n",
      "60 6\n",
      "24 25\n",
      "22 4\n",
      "13 26\n",
      "8 6\n",
      "8 8\n",
      "16 19\n",
      "19 15\n",
      "21 1\n",
      "8 4\n",
      "6 14\n",
      "8 6\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(w2v_vect[0:25]): \n",
    "    print(len(X_test.iloc[i]), len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b630b2",
   "metadata": {},
   "source": [
    "To handle this, we will average all the word vectors we have for a single message together to obtain a single word vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "287fbfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_vect_avg = []\n",
    "\n",
    "for vect in w2v_vect:\n",
    "    if len(vect)!=0:\n",
    "        w2v_vect_avg.append(vect.mean(axis=0))\n",
    "    else:\n",
    "        w2v_vect_avg.append(np.zeros(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6592c6",
   "metadata": {},
   "source": [
    "We check to make sure this fixed our problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55820adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 100\n",
      "75 100\n",
      "21 100\n",
      "12 100\n",
      "25 100\n",
      "6 100\n",
      "15 100\n",
      "25 100\n",
      "6 100\n",
      "18 100\n",
      "38 100\n",
      "43 100\n",
      "10 100\n",
      "60 100\n",
      "24 100\n",
      "22 100\n",
      "13 100\n",
      "8 100\n",
      "8 100\n",
      "16 100\n",
      "19 100\n",
      "21 100\n",
      "8 100\n",
      "6 100\n",
      "8 100\n"
     ]
    }
   ],
   "source": [
    "for i,v in enumerate(w2v_vect_avg[0:25]): \n",
    "    print(len(X_test.iloc[i]), len(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda5bce5",
   "metadata": {},
   "source": [
    "Now our features are ready to be used in a machine learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a341b336",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eab4e96",
   "metadata": {},
   "source": [
    "## Review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7050e86",
   "metadata": {},
   "source": [
    "In this notebook, we introduced Word2Vec, an embedding algorithm that takes a text corpus as input and outputs a vector representation for each word in the corpus. \n",
    "\n",
    "We saw how pre-trained embeddings could be used, as well as how embeddings could be trained using our own data. \n",
    "\n",
    "To finish up, we looked at the last bit of processing needed to be done in order to use the vector representations obtained from our word2vec model in a machine learning model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
