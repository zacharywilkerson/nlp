{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c4e90a8",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) Prerequisites\n",
    "___\n",
    "In this notebook, we will discuss some of the fundamental (or prerequisite) knowledge needed to be successful in NLP. \n",
    "\n",
    "Specifically, we'll be talking about the following:\n",
    "- [Natural Language Toolkit (NLTK)](#Natural-Language-Toolkit-(NLTK))\n",
    "- [Reading Text Data](#Reading-Text-Data)\n",
    "- [Understanding the Data](#Understanding-the-Data)\n",
    "- [Regular Expressions (RegEx)](#Regular-Expression-(RegEx))\n",
    "- [Stemming and Lemmatizing](#Stemming-and-Lemmatizing)\n",
    "\n",
    "We finish the notebook off with a review of everything discussed, as well as instructions for the assignment associated with the material covered. \n",
    "- [Review](#Review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03225da",
   "metadata": {},
   "source": [
    "> ### Natural Language Toolkit (NLTK)\n",
    "> ___\n",
    "> **NLTK** is a suite of open-source tools created to make NLP processing in Python easier. For installation instructions, see https://www.nltk.org/install.html.\n",
    ">\n",
    ">Below, we'll mess around with NLTK a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488fd8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To download packages/corpora:\n",
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45829c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see what you have installed:\n",
    "dir(nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6d22bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see stopwords (common words that don't have much meaning in the sentence):\n",
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[0:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b68fe6",
   "metadata": {},
   "source": [
    "There is plenty more, so feel free to experiment on your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbee6ec2",
   "metadata": {},
   "source": [
    "> ### Reading Text Data:\n",
    "> ___\n",
    "> Without data, we can't do any processing on it. In this section, we focus on how to load in data.\n",
    ">\n",
    "> We also take a look at **Pandas** and see how we can load data into dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909ab905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in file:\n",
    "file = open('filename.txt').read()\n",
    "\n",
    "# Print some of the data in the file:\n",
    "file[0:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d30ea3",
   "metadata": {},
   "source": [
    "Imagine each line in the file has a label followed by a sentence separated by a tab. We can break the label and sentence up to create a list that's easier to work with by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0136cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_file = file.replace('\\t','\\n').split('\\n')\n",
    "split_file[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65082e3d",
   "metadata": {},
   "source": [
    "We separate the labels and sentences as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03150d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = split_data[0::2]\n",
    "sentences = split_file[1::2]\n",
    "print('Labels:\\n',labels)\n",
    "print('Sentences:\\n',sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "662d6630",
   "metadata": {},
   "source": [
    "We will be using Pandas dataframes to work with a corpus (a collection of words). For more information on Pandas, see LINK. \n",
    "\n",
    "Here, we'll create a Pandas dataframe and . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50e0732",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "corpus = pd.DataFrame({\n",
    "    'Labels':labels,\n",
    "    'Sentences':sentences\n",
    "})\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214da47e",
   "metadata": {},
   "source": [
    "Note that Pandas can actually load in a tab-delineated file (as the one described above), so let's see how we can do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69201ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('filename.txt', sep='\\t', header=None)\n",
    "corpus.columns = ['Labels','Sentences']\n",
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5b2d7e",
   "metadata": {},
   "source": [
    "> ### Understanding the Data\n",
    "> ___\n",
    "> Now that we've loaded out data, we want to take a closer look at it. \n",
    "> \n",
    "> How is the data shaped/structured? When working with labeled data, how many samples correspond to each label? Are there more samples of one label than samples of the other label(s)? Are we missed any data? \n",
    ">\n",
    "> Let's dive in. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15e0849",
   "metadata": {},
   "source": [
    "We start by looking at the shape of our corpus, printing out the number of samples (rows) and the number of features (columns) of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff6c617",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Our corpus has {} samples with {} features.'.format(len(corpus),len(corpus.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b38bef9",
   "metadata": {},
   "source": [
    "Next, we look to see how many samples (rows) correspond to each label (first feature of our data). Assume the data we were working with had labels 'confidential' and 'non-confidential'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7585ffe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('There are {} samples labeled confidential and {} samples labeled non-confidential'.format(len(corpus['Labels']=='confidential'),\n",
    "                                                                                                 len(corpus['Labels']=='non-confidential')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc0391c",
   "metadata": {},
   "source": [
    "Now, let's see how many (if any) samples are missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a569ea23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# of missing labels: {}'.format(corpus['Labels'].isnull().sum()))\n",
    "print('# of missing sentences: {}'.format(corpus['Sentences'].isnull().sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7641baeb",
   "metadata": {},
   "source": [
    "Look at the outputs above. Was your data skewed? Was there missing data? We will want to keep that in mind moving forward.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34be6fa6",
   "metadata": {},
   "source": [
    "> ### Regular Expression (RegEx)\n",
    "> ___\n",
    "> **Regular expressions** are text strings that describe a search pattern. We will use regular expressions to simplify our data searches. \n",
    "> \n",
    "> Regular expressions will prove to be very helpful when dealing with unstructured data. In those cases, we wlll have to search for specific patterns to try and create meaning from the data. \n",
    ">\n",
    "> Here are a couple of examples that regular expressions can help us with: \n",
    "> - identifying whitespace between words/tokens\n",
    "> - removing punctuation from text\n",
    "> - cleaning HTML tags from text\n",
    "> - confirming text meets a specific criteria (such as a password)\n",
    "> - and many more! \n",
    "> \n",
    "> Some commonly used regex are shown below: \n",
    "> INSERT TABLE\n",
    "> \n",
    "> For a more extensive cheat sheet, see #. \n",
    "> \n",
    "> Below, we will use regex to **tokenize** strings. **Tokenization** refers to separating text into smaller units called tokens, which can be either words, characters, or subwords."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff74ae9d",
   "metadata": {},
   "source": [
    "To use regex in Python, we  make use of the `re` package. \n",
    "\n",
    "Let's start by importing this package and then make a few strings to use regular expressions with. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b1d3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "string1 = 'I am excited to be learning natural language processing'\n",
    "string2 = 'I        am excited to be         learning         natural language processing'\n",
    "string3 = 'I|am|excited|to|be|learning|>>>>>|natural|language|processing'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d07a5f",
   "metadata": {},
   "source": [
    "What if we wanted to split the strings up by whitespace? We can use the `re.split` method, using `'\\s'` to split the string on a whitespace character (recall the table above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "25672bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'excited', 'to', 'be', 'learning', 'natural', 'language', 'processing'] \n",
      "\n",
      "['I', '', '', '', '', '', '', '', 'am', 'excited', 'to', 'be', '', '', '', '', '', '', '', '', 'learning', '', '', '', '', '', '', '', '', 'natural', 'language', 'processing'] \n",
      "\n",
      "['I|am|excited|to|be|learning|>>>>>|natural|language|processing'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(re.split('\\s', string1),'\\n')\n",
    "print(re.split('\\s', string2),'\\n')\n",
    "print(re.split('\\s', string3),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf89914",
   "metadata": {},
   "source": [
    "We see that splitting by whitespace only helped us for `string1`. What regular expression would help us with `string2` and `string3`? \n",
    "\n",
    "Well, `string2` has multiple whitespaces in succession, so perhaps `'\\s+'` would handle those. And `string3` has non-alphanumeric characters, so perhaps `'\\W+'` would handle those. Let's try this out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d167eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'excited', 'to', 'be', 'learning', 'natural', 'language', 'processing'] \n",
      "\n",
      "['I', 'am', 'excited', 'to', 'be', 'learning', 'natural', 'language', 'processing'] \n",
      "\n",
      "['I', 'am', 'excited', 'to', 'be', 'learning', 'natural', 'language', 'processing'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(re.split('\\s', string1),'\\n')\n",
    "print(re.split('\\s+', string2),'\\n')\n",
    "print(re.split('\\W+', string3),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb737cd",
   "metadata": {},
   "source": [
    "Instead of splitting the strings, let's try using the `re.findall` method to obtain the same results. In this scenario, we will have to use negating regex compared to when we used `re.split`. Does this make sense?\n",
    "\n",
    "For example, we want to find all sets of non-whitespace characters for `string1` and `string2` (note that `string1`'s regex needs a `+` in it to get said set of characters). Similarly, we want to find all sets of alphanumeric characters for `string3`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6648e4ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'excited', 'to', 'be', 'learning', 'natural', 'language', 'processing'] \n",
      "\n",
      "['I', 'am', 'excited', 'to', 'be', 'learning', 'natural', 'language', 'processing'] \n",
      "\n",
      "['I', 'am', 'excited', 'to', 'be', 'learning', 'natural', 'language', 'processing'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(re.findall('\\S+', string1),'\\n')\n",
    "print(re.findall('\\S+', string2),'\\n')\n",
    "print(re.findall('\\w+', string3),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f839d779",
   "metadata": {},
   "source": [
    "Awesome! We just learned how to **tokenize** strings using the methods `re.split()` and `re.findall()`. \n",
    "\n",
    "There are plenty of other regex methods that will prove to be useful, such as:\n",
    "- `re.sub()`\n",
    "- `re.search()`\n",
    "- `re.match()`\n",
    "- `re.fullmatch()`\n",
    "- `re.finditer()`\n",
    "- `re.escape()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0ff60f",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af244d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d1d45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172f40a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09576ac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c84b56a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12284e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f16fe3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0bc9e15",
   "metadata": {},
   "source": [
    "> ### Review\n",
    "> ___\n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff293c0",
   "metadata": {},
   "source": [
    "Before we even start thinking about plugging out data into machine learning models, we typically have to perform these steps on our data first:\n",
    "1. Remove Punctuation\n",
    "2. Tokenize\n",
    "3. Remove Stopwords\n",
    "4. Lemmatize/Stem\n",
    "\n",
    "Assignment #1 can be used to check that we know how to do all of these things before moving on. Give it a go, making sure to compare your solution with the one given ONLY AFTER you've completed the assignment on your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4905752",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1102d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6e372e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c000873b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c2f176",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ddf1169",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
